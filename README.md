# Facial Emotion Detection 
## MIT Applied Data Science Program: Capstone Project
#### Context:
How do humans communicate with one another? While spoken and written communication may immediately come to mind, research by Dr. Albert Mehrabian has found that over 50% of communication is conveyed through body language, including facial expressions. In face-to-face conversation, body language, it turns out, plays a larger role in how our message is interpreted than both the words we choose, and the tone with which we deliver them. Our expression is a powerful window into our true feelings, and as such, it can be used as a highly-effective proxy for sentiment, particularly in the absence of written or spoken communication.

Emotion AI (artificial emotional intelligence, or affective computing), attempts to leverage this proxy for sentiment by detecting and processing facial expression (through neural networks) in an effort to successfully interpret human emotion and respond appropriately. Developing models that can accurately detect facial emotion is therefore an important driver of advancement in the realm of artificial intelligence and emotionally intelligent machines. The ability to successfully extract sentiment from images and video is also a powerful tool for businesses looking to conjure insights from the troves of unstructured data they have accumulated in recent years, or even to extract second-by-second customer responses to advertisements, store layouts, customer/user experience, etc.

#### Objective:
The objective of this project is to utilize deep learning techniques, including convolutional neural networks, to create a computer vision model that can accurately detect and interpret facial emotions. This model should be capable of performing multi-class classification on images containing one of four facial expressions: happy, sad, neutral, and surprise.

#### Key Questions:
- Do we have the data necessary to develop our models, and is it of good enough quality and quantity?
- What is the best type of machine learning model to achieve our objective?
- What do we consider 'success' when it comes to model performance?
- How do different models compare to one another given this definition of success?
- What are the most important insights that can be drawn from this project upon its conclusion?
- What is the final proposed model and is it good enough for deployment?

#### About the Dataset:
The data set consists of 3 folders, i.e., 'test', 'train', and 'validation'. Each of these folders has four subfolders:
- ‘happy’: Images of people who have happy facial expressions.
- ‘sad’: Images of people with sad or upset facial expressions.
- ‘surprise’: Images of people who have shocked or surprised facial expressions.
- ‘neutral’: Images of people showing no prominent emotion in their facial expression at all.
